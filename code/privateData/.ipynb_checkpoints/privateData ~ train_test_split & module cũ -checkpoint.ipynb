{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline # plot in cell\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# for svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "start_time1 = time.time()\n",
    "Standing = pd.read_excel(r'F:\\Program\\OneDrive\\Firefighter_AR\\Q2\\dataArticle\\newPrivate\\Standing.xlsx')\n",
    "Standing.columns\n",
    "Standing = Standing.drop(['Standing'], axis = 1)\n",
    "#Standing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sitting = pd.read_excel(r'F:\\Program\\OneDrive\\Firefighter_AR\\Q2\\dataArticle\\newPrivate\\Sitting.xlsx')\n",
    "\n",
    "Sitting.columns\n",
    "Sitting = Sitting.drop(['Time_Sitting'], axis = 1)\n",
    "Sitting.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize Sitting \n",
    "\n",
    "G = 9.81\n",
    "Sitting['x'] = Sitting['x'].div(G)\n",
    "Sitting['y'] = Sitting['y'].div(G)\n",
    "Sitting['z'] = Sitting['z'].div(G)\n",
    "\n",
    "Sitting.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Running = pd.read_excel(r'F:\\Program\\OneDrive\\Firefighter_AR\\Q2\\dataArticle\\newPrivate\\Running.xlsx')\n",
    "Running.columns\n",
    "\n",
    "Running = Running.drop(['Running','Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6',\n",
    "       'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11',\n",
    "       'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis = 1)\n",
    "Running.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Walking = pd.read_excel(r'F:\\Program\\OneDrive\\Firefighter_AR\\Q2\\dataArticle\\newPrivate\\Slow_Walking.xlsx')\n",
    "Walking.columns\n",
    "\n",
    "Walking = Walking.drop(['Slow_Walking'], axis = 1)\n",
    "Walking.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Standing))\n",
    "print(len(Sitting))\n",
    "print(len(Running))\n",
    "print(len(Walking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nan value\n",
    "Standing.isnull().sum().sum()\n",
    "Sitting.isnull().sum().sum()\n",
    "Running.isnull().sum().sum()\n",
    "Walking.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to 2 parts: Train (60%) - Test (40%)\n",
    "# Standing, Running, Walking: Fs = 50 Hz\n",
    "# window_size = 10s - 50Hz => 500 samples/frame\n",
    "\n",
    "frame = 250\n",
    "stride = 125 #step\n",
    "\n",
    "# range (start, stop, step)\n",
    "X_stand = [Standing[i:i+frame] for i in range(0, len(Standing), stride) if i+frame<=len(Standing)]\n",
    "X_run = [Running[i:i+frame] for i in range(0, len(Running), stride) if i+frame<=len(Running)]\n",
    "X_walk = [Walking[i:i+frame] for i in range(0, len(Walking), stride) if i+frame<=len(Walking)]\n",
    "\n",
    "print ('X_stand: ', len(X_stand))\n",
    "print ('X_run: ', len(X_run))\n",
    "print ('X_walk: ', len(X_walk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to 2 parts: Train (60%) - Test (40%)\n",
    "# Sitting: Fs = 15 Hz (app https://play.google.com/store/apps/details?id=com.alfav.applications.accelerometerlog&hl=en_US&gl=US&showAllReviews=true )\n",
    "# 10s - 15Hz => 150 samples/frame\n",
    "\n",
    "frame = 150\n",
    "stride = 75\n",
    "\n",
    "X_sit = [Sitting[i:i+frame] for i in range(0, len(Sitting), stride) if i+frame<=len(Sitting)]\n",
    "\n",
    "print ('X_sit: ', len(X_sit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #hàm chia ngẫu nhiên tương ứng\n",
    "RateTest = 0.4\n",
    "\n",
    "X_stand_train, X_stand_test = train_test_split(X_stand, test_size = RateTest)\n",
    "X_sit_train, X_sit_test = train_test_split(X_sit, test_size = RateTest)\n",
    "X_jog_train, X_jog_test = train_test_split(X_jog, test_size = RateTest)\n",
    "X_walk_train, X_walk_test = train_test_split(X_walk, test_size = RateTest)\n",
    "\n",
    "\n",
    "print('train =  ', len(X_stand_train)+ len(X_sit_train) + len(X_jog_train) + len(X_walk_train))\n",
    "print('test =  ', len(X_stand_test)+ len(X_sit_test) + len(X_jog_test) + len(X_walk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "test_data = []\n",
    "test_label = []\n",
    "\n",
    "for acts in X_stand_train:\n",
    "    train_data.append(acts)\n",
    "    train_label.append(0)\n",
    "    \n",
    "for acts in X_sit_train:\n",
    "    train_data.append(acts)\n",
    "    train_label.append(1)\n",
    "    \n",
    "for acts in X_run_train:\n",
    "    train_data.append(acts)\n",
    "    train_label.append(2)\n",
    "\n",
    "for acts in X_walk_train:\n",
    "    train_data.append(acts)\n",
    "    train_label.append(3)\n",
    "\n",
    "print('train-data length: ', len(train_data) )\n",
    "print('train-label length: ', len(train_label) )\n",
    "#print(train_label)\n",
    "      \n",
    "# For TEST\n",
    "\n",
    "for acts in X_stand_test:\n",
    "    test_data.append(acts)\n",
    "    test_label.append(0)\n",
    "\n",
    "for acts in X_sit_test:\n",
    "    test_data.append(acts)\n",
    "    test_label.append(1)\n",
    "        \n",
    "for acts in X_run_test:\n",
    "    test_data.append(acts)\n",
    "    test_label.append(2)\n",
    "\n",
    "for acts in X_walk_test:\n",
    "    test_data.append(acts)\n",
    "    test_label.append(3)\n",
    "\n",
    "print('test-data length: ', len(test_data))\n",
    "print('test-label length: ', len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diffState(x,y,z):\n",
    "#     result = 0\n",
    "#     for i,j,k in zip(x,y,z):\n",
    "#         summ = pow((i-np.mean(x)),2)+pow((j-np.mean(y)),2)+pow((k-np.mean(z)),2)\n",
    "#         result = result + summ\n",
    "#     return np.sqrt(result)\n",
    "\n",
    "def featuresFromBuffer(at):\n",
    "    feat = np.zeros(15)   \n",
    "    \n",
    "    x = np.array(at.iloc[:,0], dtype=np.float64)   \n",
    "    y = np.array(at.iloc[:,1], dtype=np.float64)   \n",
    "    z = np.array(at.iloc[:,2], dtype=np.float64)  \n",
    "    \n",
    "    means = [np.mean(i) for i in [x, y, z]]\n",
    "    feat[0:3] = means \n",
    "    \n",
    "    rms = [np.sqrt(np.mean(i**2)) for i in [x, y, z]]\n",
    "    feat[3:6] = rms\n",
    "\n",
    "    # Standard deviation\n",
    "    std = [np.std(i) for i in [x, y, z]]\n",
    "    feat[6:9] = std\n",
    "\n",
    "    # Median\n",
    "    med = [np.median(i) for i in [x, y, z]]\n",
    "    feat[9:12] = med\n",
    "    \n",
    "    # Range\n",
    "    Range = [np.amax(i)-np.amin(i) for i in [x, y, z]]    \n",
    "    feat[12:15] = Range\n",
    "    \n",
    "#     feat[15] = diffState(x,y,z)\n",
    "    \n",
    "    return feat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo mảng features \n",
    "\n",
    "train_features = []\n",
    "test_features = []\n",
    "for action in train_data:\n",
    "    feat = featuresFromBuffer(action)\n",
    "    train_features.append(feat)  \n",
    "\n",
    "for action in test_data:\n",
    "    feat = featuresFromBuffer(action)\n",
    "    test_features.append(feat)\n",
    "\n",
    "len(train_features)\n",
    "len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower accuracy (nan value -> 0)\n",
    "np.where(np.isnan(train_features)) \n",
    "train_features = np.nan_to_num(train_features)\n",
    "np.where(np.isnan(test_features))\n",
    "test_features = np.nan_to_num(test_features)      \n",
    "\n",
    "len(train_features)\n",
    "len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier \n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "print(\"Gradient Boosting Decision Tree:\")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf1 = GradientBoostingClassifier(learning_rate=0.05,max_depth=3,n_estimators=100).fit(train_features, train_label)\n",
    "#format: pass score in {:.3f}\n",
    "print('Accuracy of GBDT classifier on training set: {:.3f}'\n",
    "     .format(clf1.score(train_features, train_label)))\n",
    "print('Accuracy of GBDT classifier on test set: {:.3f}'\n",
    "     .format(clf1.score(test_features, test_label)))\n",
    "\n",
    "print(\"\\n\\nDecision Tree:\") \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree \n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth=4).fit(train_features, train_label)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.3f}'.format(clf2.score(train_features, train_label)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.3f}'\n",
    ".format(clf2.score(test_features, test_label)))\n",
    "\n",
    "\n",
    "print(\"\\n\\nSVM:\")\n",
    "clf3 = SVC(C=100, gamma='scale').fit(train_features, train_label)\n",
    "print(\"Accuracy on training set: {:.2f}\".format(clf3.score(train_features, train_label)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(clf3.score(test_features, test_label)))\n",
    "\n",
    "\n",
    "print('\\n\\nRandom Forests: ')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf4 = RandomForestClassifier(n_estimators=200, random_state=0).fit(train_features, train_label)\n",
    "\n",
    "print('Accuracy of RF classifier on training set: {:.3f}'\n",
    "     .format(clf4.score(train_features, train_label)))\n",
    "print('Accuracy of RF classifier on test set: {:.3f}'\n",
    "     .format(clf4.score(test_features, test_label)))\n",
    "\n",
    "\n",
    "print('\\n\\nKNeighbor: ')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, weights = 'distance').fit(train_features, train_label)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(train_features, train_label)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(test_features, test_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "      \n",
    "y_pred = clf4.predict(test_features)\n",
    "\n",
    "print('micro')\n",
    "f1_score(test_label, y_pred, average='micro')\n",
    "recall_score(test_label, y_pred, average='micro')\n",
    "precision_score(test_label, y_pred, average='micro',labels=np.unique(y_pred))\n",
    "\n",
    "#overall measurement\n",
    "print('macro')\n",
    "f1_score(test_label, y_pred, average='macro')\n",
    "recall_score(test_label, y_pred, average='macro')\n",
    "precision_score(test_label, y_pred, average='macro',labels=np.unique(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sb\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "target_names = ['Standing', 'Sitting', 'Running', 'Walking']\n",
    "\n",
    "y_pred = clf4.predict(test_features)\n",
    "lables = np.concatenate([target_names, ])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False, title=None):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    #metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (10,6))\n",
    "    #Create a 2-dim array include: rows, columns, data\n",
    "    dataFrame = pd.DataFrame(cm)\n",
    "    #create a heatmap, annot = True: add text on each cell over heatmap\n",
    "    sb.heatmap(dataFrame, annot = True)  \n",
    "    ax.set(xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "\n",
    "    sb.set(font_scale = 1.3)\n",
    "\n",
    "np.set_printoptions(precision=2) #lam tron\n",
    "\n",
    "plot_confusion_matrix(test_label, y_pred, classes=lables,  \n",
    "        title='GDBT \\nConfusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(test_label, y_pred, classes=lables, normalize=True,\n",
    "        title='GDBT \\nNormalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print (\"\\nClassification Report: \")\n",
    "print (classification_report(test_label, y_pred,labels=np.unique(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time \n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bita6c16f8bd0f8458dae0b3c3b36181794"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
